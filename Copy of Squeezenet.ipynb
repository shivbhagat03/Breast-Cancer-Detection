{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"12NObXK9qUv2dyeo0WPRRST3XqnPVdXX4","timestamp":1731351165282}],"gpuType":"V28","authorship_tag":"ABX9TyPXyqjMd4HzsxUAqCeKMWls"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["# Import necessary libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","import numpy as np\n","import os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import jaccard_score\n","from google.colab import drive"],"metadata":{"id":"lO0bJBZWU4AC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# connect the google drive to you colab for data acess\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wOGIFwWNVNp0","executionInfo":{"status":"ok","timestamp":1731383524172,"user_tz":-330,"elapsed":3436,"user":{"displayName":"Belly","userId":"05450581440547444310"}},"outputId":"2637e610-023f-47b2-c2b6-2622653e96da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Update mask transform to resize masks to 104x104 to match model's output\n","class MedicalImageDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.image_transform = image_transform\n","        self.mask_transform = mask_transform\n","        self.image_files = os.listdir(image_dir)\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        image_path = os.path.join(self.image_dir, self.image_files[idx])\n","        mask_path = os.path.join(self.mask_dir, self.image_files[idx])  # Assuming mask has the same filename\n","\n","        image = Image.open(image_path).convert(\"RGB\")\n","        mask = Image.open(mask_path).convert(\"L\")\n","\n","        if self.image_transform:\n","            image = self.image_transform(image)\n","        if self.mask_transform:\n","            mask = self.mask_transform(mask)\n","\n","        return image, mask\n","\n","# Define transformations for images and masks\n","image_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor()\n","])\n","\n","mask_transform = transforms.Compose([\n","    transforms.Resize((104, 104)),  # Resize mask to match model's current output size\n","    transforms.ToTensor()\n","])\n","\n","# Load dataset with separate transforms for images and masks\n","train_dataset = MedicalImageDataset(image_dir, mask_dir, image_transform=image_transform, mask_transform=mask_transform)\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n"],"metadata":{"id":"bdzIt-8zVHto"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SqueezeNetSegmentation(nn.Module):\n","    def __init__(self, num_classes=1):  # Binary segmentation\n","        super(SqueezeNetSegmentation, self).__init__()\n","        squeezenet = models.squeezenet1_1(pretrained=True)\n","\n","        # Keep the feature extraction part of SqueezeNet\n","        self.features = nn.Sequential(*list(squeezenet.features.children())[:-1])\n","\n","        # Add upsampling layers for segmentation\n","        # Adjusted kernel_size and stride in ConvTranspose2d layers\n","        # to achieve the desired output size of 224x224\n","        self.upsample = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # Adjusted\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # Adjusted\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(64, num_classes, kernel_size=4, stride=2, padding=1),  # Adjusted\n","            nn.Sigmoid()  # For binary segmentation\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.upsample(x)\n","        return x"],"metadata":{"id":"a91LCs1HXJlj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss function and optimizer\n","model = SqueezeNetSegmentation()\n","criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"],"metadata":{"id":"PQbZ7rCsXOj8","executionInfo":{"status":"ok","timestamp":1731383524173,"user_tz":-330,"elapsed":5,"user":{"displayName":"Belly","userId":"05450581440547444310"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"90d1e5ec-accf-4d4c-9812-26a8a82a0301"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["def refine_breast_area(mask, min_area_threshold=5000):\n","    print(f\"Refining mask with area threshold {min_area_threshold}...\")\n","    mask = (mask * 255).astype(np.uint8)\n","\n","    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    refined_mask = np.zeros_like(mask)\n","\n","    for contour in contours:\n","        area = cv2.contourArea(contour)\n","        if area >= min_area_threshold:\n","            print(f\"Retaining contour with area: {area}\")\n","            cv2.drawContours(refined_mask, [contour], -1, (255), thickness=cv2.FILLED)\n","\n","    refined_mask = (refined_mask > 0).astype(np.uint8)\n","    print(\"Mask refinement complete.\")\n","    return refined_mask\n"],"metadata":{"id":"Nr9upe43lUg7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","def train(model, loader, criterion, optimizer, epochs=10):\n","    model.train()\n","    for epoch in range(epochs):\n","        epoch_loss = 0\n","        for images, masks in loader:\n","            images, masks = images.to(device), masks.to(device)\n","\n","            # Forward pass\n","            outputs = model(images)\n","            loss = criterion(outputs, masks)\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","\n","        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss / len(loader)}\")\n","\n","# Start training\n","train(model, train_loader, criterion, optimizer, epochs=10)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dE0CoJfuXTC0","executionInfo":{"status":"ok","timestamp":1731383972577,"user_tz":-330,"elapsed":448407,"user":{"displayName":"Belly","userId":"05450581440547444310"}},"outputId":"2ad775d8-98b4-4a43-fc32-3cbb978c8b79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 0.3884918871853087\n","Epoch [2/10], Loss: 0.1335462292449342\n","Epoch [3/10], Loss: 0.08776108589437273\n","Epoch [4/10], Loss: 0.0651119156844086\n","Epoch [5/10], Loss: 0.05618860189699464\n","Epoch [6/10], Loss: 0.050617175073259406\n","Epoch [7/10], Loss: 0.050209627486765385\n","Epoch [8/10], Loss: 0.04277064268373781\n","Epoch [9/10], Loss: 0.03880380766673221\n","Epoch [10/10], Loss: 0.03658887878474262\n"]}]},{"cell_type":"code","source":["def evaluate_with_refinement(model, loader, min_area_threshold=5000):\n","    model.eval()\n","    dice_scores = []\n","    jaccard_indices = []\n","\n","    with torch.no_grad():\n","        for images, masks in loader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            # Forward pass to get predictions\n","            outputs = model(images)\n","            preds = outputs > 0.5  # Threshold to get binary predictions\n","\n","            # Convert predictions to numpy for post-processing\n","            preds_np = preds.squeeze(1).cpu().numpy()\n","\n","            # Apply post-processing refinement to each mask in the batch\n","            refined_preds = np.array([refine_breast_area(pred, min_area_threshold) for pred in preds_np])\n","\n","            # Convert refined predictions back to tensors for metric calculation\n","            refined_preds = torch.from_numpy(refined_preds).to(device)\n","\n","            # Calculate metrics (Dice and Jaccard)\n","            for i in range(len(refined_preds)):\n","                pred_flat = refined_preds[i].flatten()\n","                mask_flat = masks[i].cpu().numpy().flatten()\n","\n","                intersection = (pred_flat * mask_flat).sum()\n","                dice = (2 * intersection) / (pred_flat.sum() + mask_flat.sum() + 1e-7)\n","                dice_scores.append(dice)\n","\n","                jaccard = jaccard_score(mask_flat, pred_flat, average='binary')\n","                jaccard_indices.append(jaccard)\n","\n","    avg_dice = np.mean(dice_scores)\n","    avg_jaccard = np.mean(jaccard_indices)\n","    print(f\"Avg Dice Coefficient: {avg_dice}, Avg Jaccard Index: {avg_jaccard}\")\n","    return avg_dice, avg_jaccard\n"],"metadata":{"id":"mnMkCISMczYu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install opencv-python\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v8X35CGbiqcO","executionInfo":{"status":"ok","timestamp":1731384233391,"user_tz":-330,"elapsed":5363,"user":{"displayName":"Belly","userId":"05450581440547444310"}},"outputId":"26a9d69b-ee3c-4743-f219-73dc9442b2e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n"]}]},{"cell_type":"code","source":["import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def refine_breast_area(mask, min_area_threshold=5000):\n","    \"\"\"\n","    Refine the segmentation mask to focus on the breast area by removing small unwanted regions.\n","\n","    Parameters:\n","    mask (numpy.ndarray): Binary mask where the breast area is 1, and background is 0.\n","    min_area_threshold (int): Minimum area threshold for retaining a region.\n","\n","    Returns:\n","    numpy.ndarray: Refined binary mask with unwanted areas removed.\n","    \"\"\"\n","    # Convert mask to uint8 for OpenCV processing\n","    mask = (mask * 255).astype(np.uint8)\n","\n","    # Find contours in the mask\n","    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Create an empty mask to hold refined regions\n","    refined_mask = np.zeros_like(mask)\n","\n","    # Retain only large contours based on min_area_threshold\n","    for contour in contours:\n","        area = cv2.contourArea(contour)\n","        if area >= min_area_threshold:\n","            cv2.drawContours(refined_mask, [contour], -1, (255), thickness=cv2.FILLED)\n","\n","    # Convert back to binary mask (0 and 1)\n","    refined_mask = (refined_mask > 0).astype(np.uint8)\n","    return refined_mask\n","\n","def overlay_mask_on_image(image, mask, alpha=0.5):\n","    \"\"\"\n","    Overlay a binary mask on an image with transparency.\n","\n","    Parameters:\n","    image (numpy.ndarray): The original image as a NumPy array.\n","    mask (numpy.ndarray): The binary mask to overlay, as a NumPy array.\n","    alpha (float): Transparency level for the overlay.\n","\n","    Returns:\n","    numpy.ndarray: Image with the mask overlayed.\n","    \"\"\"\n","    overlay = image.copy()\n","    overlay[mask == 1] = [255, 0, 0]  # Color the mask region red\n","\n","    # Blend the original image and mask overlay\n","    return cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)\n","\n","def test_and_visualize_with_roi(model, test_loader, min_area_threshold=5000):\n","    \"\"\"\n","    Run the model on test data and visualize the refined ROI mask overlayed on the original image.\n","\n","    Parameters:\n","    model (torch.nn.Module): The trained model.\n","    test_loader (DataLoader): DataLoader for the test dataset.\n","    min_area_threshold (int): Minimum area threshold for keeping contours in the mask.\n","    \"\"\"\n","    model.eval()\n","    with torch.no_grad():\n","        for i, (image, mask) in enumerate(test_loader):\n","            image = image.to(device)\n","            mask = mask.to(device)\n","            output = model(image)\n","            pred = output > 0.5  # Binary thresholding\n","\n","            # Convert the prediction to a numpy array for post-processing\n","            pred_np = pred.squeeze(1).cpu().numpy()\n","\n","            # Refine the mask to focus on the main breast area\n","            refined_preds = np.array([refine_breast_area(pred, min_area_threshold) for pred in pred_np])\n","\n","            # Display results with mask overlay\n","            for j in range(len(refined_preds)):\n","                img = image[j].cpu().numpy().transpose(1, 2, 0)  # Convert tensor to HWC format\n","                img = (img * 255).astype(np.uint8)  # Scale image to [0, 255] for display\n","                mask_gt = mask[j].cpu().squeeze().numpy()  # Ground truth mask\n","                refined_pred = refined_preds[j]  # Refined mask from the model\n","\n","                # Overlay the refined mask on the original image\n","                overlay_img = overlay_mask_on_image(img, refined_pred)\n","\n","                # Display the images\n","                plt.figure(figsize=(15, 5))\n","                plt.subplot(1, 4, 1)\n","                plt.imshow(img, cmap='gray')\n","                plt.title(\"Original Image\")\n","                plt.subplot(1, 4, 2)\n","                plt.imshow(mask_gt, cmap='gray')\n","                plt.title(\"Ground Truth Mask\")\n","                plt.subplot(1, 4, 3)\n","                plt.imshow(refined_pred, cmap='gray')\n","                plt.title(\"Refined Predicted Mask\")\n","                plt.subplot(1, 4, 4)\n","                plt.imshow(overlay_img)\n","                plt.title(\"ROI Overlay on Original Image\")\n","                plt.show()\n","            break  # Display one batch for example\n"],"metadata":{"id":"Ak7zJ6ZJitde"},"execution_count":null,"outputs":[]}]}